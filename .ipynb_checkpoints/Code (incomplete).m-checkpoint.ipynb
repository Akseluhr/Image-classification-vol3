{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "\n",
    "# Matlab -> Python functions\n",
    "\n",
    "###############################################################\n",
    "\n",
    "# Loades an entire batch\n",
    "def LoadBatch(filename):\n",
    "\t\"\"\" Copied from the dataset website \"\"\" \n",
    "\twith open('Datasets/'+filename, 'rb') as fo:\n",
    "\t\tdict = pickle.load(fo, encoding='bytes') \n",
    "\treturn dict\n",
    "\n",
    "# Calculate softmax for class class estimation of each image (vector)\n",
    "def softmax(x):\n",
    "\t\"\"\" Standard definition of the softmax function \"\"\"\n",
    "\texp_x = np.exp(x)\n",
    "\treturn exp_x / np.sum(exp_x, axis=0)\n",
    "\n",
    "#\n",
    "def ComputeGradsNumSlow(X, Y, W, b, lamba, h):\n",
    "    \"\"\"\n",
    "    Compute gradients numerically via the centred difference method\n",
    "    :param X: numpy array of shape dxN, batch of N input images\n",
    "    :param Y: numpy array of shape KxN, batch of N one-hot image labels\n",
    "    :param W: list of nxm numpy arrays, containing (in order) the weights of the first layer, then the second, etc.\n",
    "    :param b: list of nx1 numpy arrays, containing (in order) the biases of the first layer, then the second, etc.\n",
    "    :param lamba: float, lambda parameter for loss function\n",
    "    :param h: float, step size for numerical analysis\n",
    "    :return: list of numpy arrays grad_W, grad_b, of same format as W and b, respectively\n",
    "    \"\"\"\n",
    "    grad_W = [np.zeros(w_n.shape) for w_n in W]\n",
    "    grad_b = [np.zeros(b_n.shape) for b_n in b]\n",
    "\n",
    "    for j in range(len(grad_b)):\n",
    "        bj_size = grad_b[j].shape[0]  #\n",
    "        for i in range(bj_size):\n",
    "            b_try = [bj.copy() for bj in b]\n",
    "            b_try[j][i] -= h\n",
    "            c1 = ComputeCost(X, Y, W, b_try, lamba)\n",
    "\n",
    "            b_try = [bj.copy() for bj in b]\n",
    "            b_try[j][i] += h\n",
    "            c2 = ComputeCost(X, Y, W, b_try, lamba)\n",
    "\n",
    "            grad_b[j][i] = (c2 - c1) / (2*h)\n",
    "\n",
    "    for j in range(len(grad_W)):\n",
    "        Wj = grad_W[j]\n",
    "        for i in range(Wj.shape[0]):\n",
    "            for k in range(Wj.shape[1]):\n",
    "                W_try = [wj.copy() for wj in W]\n",
    "                W_try[j][i,k] -= h\n",
    "                c1 = ComputeCost(X, Y, W_try, b, lamba)\n",
    "\n",
    "                W_try = [wj.copy() for wj in W]\n",
    "                W_try[j][i,k] += h\n",
    "                c2 = ComputeCost(X, Y, W_try, b, lamba)\n",
    "                grad_W[j][i,k] = (c2 - c1) / (2*h)\n",
    "\n",
    "    return grad_W, grad_b\n",
    "\n",
    "\n",
    "# Allows for efficiently view the images in a directory or \n",
    "# in a *Matlab* array or cell array\n",
    "def montage(W):\n",
    "\t\"\"\" Display the image for each label in W \"\"\"\n",
    "\timport matplotlib.pyplot as plt\n",
    "\tfig, ax = plt.subplots(2,5)\n",
    "\tfor i in range(2):\n",
    "\t\tfor j in range(5):\n",
    "\t\t\tim  = W[i*5+j,:].reshape(32,32,3, order='F')\n",
    "\t\t\tsim = (im-np.min(im[:]))/(np.max(im[:])-np.min(im[:]))\n",
    "\t\t\tsim = sim.transpose(1,0,2)\n",
    "\t\t\tax[i][j].imshow(sim, interpolation='nearest')\n",
    "\t\t\tax[i][j].set_title(\"y=\"+str(5*i+j))\n",
    "\t\t\tax[i][j].axis('off')\n",
    "\tplt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "\n",
    "# My functions\n",
    "\n",
    "###############################################################\n",
    "\n",
    "# Read pixel data, labels (classes), one-hot rep. of labels (classes)\n",
    "# Divide pixel data by 255 for correct format\n",
    "def ReadData(filename):\n",
    "    data_batch = LoadBatch(filename)\n",
    "    pixel_data = data_batch[b'data'].T\n",
    "    labels = data_batch[b'labels']\n",
    "    one_hot = np.eye(10)[labels].T\n",
    "    return pixel_data, one_hot, labels \n",
    "\n",
    "# Normalize w.r.t. training data mean and standard deviation\n",
    "# Normalization of input so that the inputs are at a comparable range\n",
    "def Normalize(train, validation,test=None):\n",
    "        train=np.float64(train)\n",
    "        validation=np.float64(validation)\n",
    "        \n",
    "        mean_X =train.mean(axis=1)\n",
    "\n",
    "        std_X=train.std(axis=1)\n",
    "\n",
    "        train=train-mean_X[:,None]\n",
    "\n",
    "        train=train/std_X[:,None]\n",
    "        validation=validation-mean_X[:,None]\n",
    "        validation=validation/std_X[:,None]\n",
    "        \n",
    "        if(test is not None):\n",
    "            test=np.float64(test)\n",
    "            test=test-mean_X[:,None]\n",
    "            test=test/std_X[:,None]\n",
    "            return train,validation,test;\n",
    "        \n",
    "        return train,validation;\n",
    "\n",
    "# First init of model params W(eights) and b(ias)\n",
    "# Init done with 0 mean and 1 / sqrt of d and m\n",
    "# Random seed for selecting the same rndm numbers for each execution\n",
    "def GetWeightAndBias(X, Y, k, m=50):\n",
    "    \n",
    "    weights = list()\n",
    "    bias = list()\n",
    "    \n",
    "    # dimension (3072)\n",
    "    d = X.shape[0]\n",
    "    # dimension, not layers\n",
    "    K = 10\n",
    "\n",
    "    # std to avoid exploding gradients\n",
    "    std_d = 1 / np.sqrt(d)\n",
    "    std_m = 1 / np.sqrt(m)\n",
    "    \n",
    "    np.random.seed(400)\n",
    "    \n",
    "    # First weight, 50 x 3072\n",
    "    weights.append(np.random.normal(loc=0.0, scale=std_d, size=(m, d)))\n",
    "    # First bias, 50 x 1\n",
    "    bias.append(np.random.normal(loc=0.0, scale=std_d, size=(m,1)))\n",
    "\n",
    "    # Rest of weight and bias:\n",
    "    # 50 x 50 (W)\n",
    "    # 50 x 1 (b)\n",
    "    for i in range(k):\n",
    "        np.random.seed(400)\n",
    "        weights.append(np.random.normal(loc=0.0, scale=std_d, size=(m, m)))\n",
    "        bias.append(np.random.normal(loc=0.0, scale=std_d, size=(m,1)))\n",
    "\n",
    "    \n",
    "    np.random.seed(400)\n",
    "    # Add weights to list\n",
    "    weights.append(np.random.normal(loc=0.0, scale=std_m, size=(K, m)))\n",
    "        \n",
    "    # Add bias to list\n",
    "    bias.append(np.random.normal(loc=0.0, scale=std_m, size=(K,1)))\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "# Evaluation of the network function\n",
    "# Agan, Softmax returns each probability for each class label\n",
    "def EvaluateClassifier(X, W, b, k):\n",
    "\n",
    "    h_list = list()\n",
    "    # Output first layer\n",
    "    si = np.dot(W[0], X) + b[0]\n",
    "    h_list.append(np.maximum(0,si))\n",
    "    \n",
    "    for i in range(1, k+1):  \n",
    "        # Check output values using relu (activation)\n",
    "        # This will be input to the next layer\n",
    "        si = np.dot(W[i], h_list[i-1]) + b[i]\n",
    "        h_list.append(np.maximum(0,si))\n",
    "        \n",
    "    # Final layer (output layer)\n",
    "    s = np.dot(W[-1], h_list[-1]) + b[-1]\n",
    "    P = softmax(s)\n",
    "    \n",
    "    return P, h_list\n",
    "\n",
    "# Total cost of a set of images:\n",
    "# 1. Regularization term, calculate: lambda * sum(W^2 ij)\n",
    "# 2. Sum it with l_cross + regularization term -> for each x,y in D\n",
    "# 3. Multiply everything with 1 / length of D\n",
    "def ComputeCost(X, Y, W, b, lambd, k=2):\n",
    "    \n",
    "    P, H = EvaluateClassifier(X, W, b, k)\n",
    "    hej = 0 \n",
    "    for i in range(len(W)):\n",
    "        hej += (W[i]**2).sum()\n",
    "        \n",
    "    lcr = -np.sum(np.multiply(Y, np.log(P)))\n",
    "    Reg_term = lambd*hej\n",
    "    J = lcr/X.shape[1]+Reg_term\n",
    "    \n",
    "    return J\n",
    "\n",
    "\n",
    "# Accuracy of the network's predictions\n",
    "# Percentage of examples for which it gets the correct answer\n",
    "def ComputeAccuracy(X, y, W, b, k):\n",
    "    P, act_vals = EvaluateClassifier(X, W, b, k)\n",
    "    acc = np.mean(y == np.argmax(P, axis=0))\n",
    "    \n",
    "    return acc\n",
    "    \n",
    "# Compute gradients of the cost function to see the curve of cost decrease \n",
    "# Forward pass is already done since we have already calculated P\n",
    "def ComputeGradients(h_vals, X, Y, P, W,lambd, k):\n",
    "\n",
    "    grad_Ws = list()\n",
    "    grad_bs = list()\n",
    "    n_b1 = X.shape[1]\n",
    "    print(X.shape[1])\n",
    "    print(h_vals[0].shape[1])\n",
    "    K = Y.shape[0]\n",
    "    d = h_vals[0].shape[0]\n",
    "    print(\"d: \", h_vals[0].shape[0])\n",
    "    # Backward pass\n",
    "    g = -(Y - P)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Append Wk, bk\n",
    "    for i in range(len(W)-1, 0, -1):\n",
    "        grad_Ws.append(1 / n_b1 * np.dot(g, h_vals[i-1].T) + 2 * lambd * W[i])\n",
    "        grad_bs.append(1/n_b1 * np.dot(g, np.ones(shape=(n_b1,1))))\n",
    "        g = np.dot(W[i].T, g)\n",
    "        g = g*(h_vals[i-1] > 0)\n",
    "    \n",
    "    # Appedn W1, b1\n",
    "    grad_Ws.append(1 / n_b1 * np.dot(g, X.T) + 2 * lambd * W[0])\n",
    "    grad_bs.append((np.dot(g,np.ones(shape=(n_b1,1)))/n_b1).reshape(d,1))\n",
    "    \n",
    "    return grad_Ws, grad_bs\n",
    "\n",
    "# Check if my analytical gradients \n",
    "# Using centered difference function\n",
    "# If the differenc is < 1e-6, the analytical gradients are fine\n",
    "def CompareGradients(X,Y, W, b, lambd, k, threshold):\n",
    "    \n",
    "    P, H = EvaluateClassifier(X, W, b, k)\n",
    "\n",
    "    #Calculate gradients X, Y, W, b, lamba, h)\n",
    "    grad_Ws_a, grad_bs_a = ComputeGradients(H, X, Y,P, W, lambd, k)\n",
    "    grad_Ws_n, grad_bs_n = ComputeGradsNumSlow(X, Y, W, b, lambd, h=0.00001)\n",
    "\n",
    "    print(grad_Ws_a[0].shape, \"www\")\n",
    "    print(grad_Ws_n[0].shape, \"ddd\")\n",
    "    w_rel_error = list()\n",
    "    b_rel_error = list()\n",
    "    # Calculate differences\n",
    "    for i in range(len(W)):\n",
    "        w_rel_error.append(np.sum(np.abs(grad_Ws_a[i] - grad_Ws_n[i].T)) / np.maximum(0.001, np.sum(np.abs(grad_Ws_a[i]) + np.abs(grad_Ws_n[i].T))))\n",
    "        b_rel_error.append(np.sum(np.abs(grad_bs_a[i] - grad_bs_n[i].T)) / np.maximum(0.001, np.sum(np.abs(grad_bs_a[i]) + np.abs(grad_bs_n[i].T))))\n",
    "\n",
    "        print(w_rel_error[i])\n",
    "        print(b_rel_error[i])\n",
    "        # Check differences    \n",
    "        if (w_rel_error[i] and b_rel_error[i]) < threshold:\n",
    "            print(\"Analytical ok\")\n",
    "        else:\n",
    "            print(\"Gradient difference too high\")\n",
    "\n",
    " \n",
    "def MiniBatchGD2(X, Y, y, GDparams, W1, W2, b1, b2, X_val=None, Y_val=None, y_val=None, lambd= 0 ):\n",
    "    n = X.shape[1]\n",
    "    (eta_min,eta_max,step_size,n_batch,cycles)=GDparams\n",
    "    metrics = {'updates':[-1], \n",
    "               'Loss_scores':[ComputeCost(X, Y, W1, W2, b1, b2, lambd)], \n",
    "               'acc_scores':[ComputeAccuracy(X, y, W1, W2, b1, b2)]}\n",
    "    if X_val is not None:\n",
    "        metrics['Loss_val_scores'] = [ComputeCost(X_val, Y_val, W1, W2,b1, b2, lambd)]\n",
    "        metrics['acc_val_scores'] = [ComputeAccuracy(X_val, y_val, W1, W2, b1, b2)]\n",
    "    batches = dict()\n",
    "\n",
    "    for j in range(n//n_batch):\n",
    "            j_start = (j)*n_batch ;\n",
    "            j_end = (j+1)*n_batch;\n",
    "            inds = range(j_start,j_end);\n",
    "            y_batch = [y[index] for index in inds]\n",
    "            X_batch = X[:, inds];\n",
    "            Y_batch = Y[:, inds];\n",
    "            batches[j]=(X_batch,Y_batch,y_batch)\n",
    "    j = 0\n",
    "    \n",
    "    for l in range(cycles):\n",
    "        for t in range(2*l*step_size, 2*(l+1)*step_size):\n",
    "            \n",
    "            if t>= 2*l*step_size and t<(2*l+1)*step_size:\n",
    "                eta = eta_min+(t-2*l*step_size)/step_size*(eta_max-eta_min)\n",
    "            elif t>=(2*l+1)*step_size and t<2*(l+1)*step_size:\n",
    "                eta = eta_max-(t-(2*l+1)*step_size)/step_size*(eta_max-eta_min)\n",
    "\n",
    "            X_batch, Y_batch, y_batch = batches[j]\n",
    "            P_batch, H_batch = EvaluateClassifier(X_batch, W1, W2,b1, b2)\n",
    "            grad_W1, grad_W2, grad_b1, grad_b2 = ComputeGradients(H_batch, X_batch, Y_batch, P_batch, W1, W2,lambd)\n",
    "\n",
    "          #  print(W1)\n",
    "           # if(math.isnan(W1[0][0])):\n",
    "           #     print(\"NU ÄR DET NAN \")\n",
    "           #     print(\"J = \", j)\n",
    "           #     print(W1)\n",
    "             #   hej()\n",
    "                # .1 * 1.5\n",
    "                # .0001 * 1.5\n",
    "            W1 -= eta*grad_W1\n",
    "            b1 -= eta*grad_b1\n",
    "            W2 -= eta*grad_W2\n",
    "            b2 -= eta*grad_b2\n",
    "            j += 1\n",
    "            if j>(n//n_batch-1):\n",
    "                # set j = 0 will start new epoch\n",
    "                j = 0\n",
    "                metrics['updates'].append(t+1)\n",
    "                metrics['acc_scores'].append(ComputeAccuracy(X, y, W1, W2, b1, b2))\n",
    "                metrics['Loss_scores'].append(ComputeCost(X, Y, W1, W2, b1, b2,lambd))\n",
    "\n",
    "                if X_val is not None:\n",
    "                    metrics['acc_val_scores'].append(ComputeAccuracy(X_val, y_val, W1, W2,b1, b2))\n",
    "                    metrics['Loss_val_scores'].append(ComputeCost(X_val, Y_val, W1, W2,b1, b2, lambd))\n",
    "                message = \"In update \"+str(t+1)+'/'+str(2*cycles*step_size)+\" finishes epoch \"+ \\\n",
    "                          str(len(metrics['updates'])-1)+\": loss=\"+str(metrics['Loss_scores'][-1])+ \\\n",
    "                          \" and accuracy=\"+str(metrics['acc_scores'][-1])+\" (training set) \\r\"\n",
    "                sys.stdout.write(message)\n",
    "            \n",
    "        \n",
    "    \n",
    "    return W1, b1, W2, b2, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  2.3196893449321383\n",
      "Accuracy:  0.103\n",
      "10000\n",
      "10000\n",
      "d:  50\n",
      "1\n",
      "1\n",
      "d:  50\n",
      "(10, 50) www\n",
      "(50, 10) ddd\n",
      "0.8981875005891061\n",
      "0.9945927158281133\n",
      "Gradient difference too high\n",
      "0.9305912974461756\n",
      "0.9517472732408547\n",
      "Gradient difference too high\n",
      "0.9305912972551318\n",
      "0.9517472732501319\n",
      "Gradient difference too high\n",
      "0.8981875003279501\n",
      "0.9945927158277\n",
      "Gradient difference too high\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "X_train, Y_train, y_test = ReadData('data_batch_1')\n",
    "X_val_train, Y_val_train, y_val_test = ReadData('data_batch_2')\n",
    "X_test_train, Y_test_train, y_test_test = ReadData('test_batch')\n",
    "\n",
    "# Normalize all data w.r.t. mean and std of training data\n",
    "X_train_normalized, X_val_train_normalized, X_test_train_normalized = Normalize(X_train, X_val_train, X_test_train)\n",
    "\n",
    "# Create model params W and b\n",
    "W, b = GetWeightAndBias(X_train_normalized, Y_train, 2, m=50)\n",
    "\n",
    "# Model evaluation (softmax)\n",
    "P, act_vals = EvaluateClassifier(X_train_normalized, W, b, 2)\n",
    "\n",
    "# Cost function\n",
    "lambd = 0.0\n",
    "A = ComputeCost(X_train_normalized, Y_train, W, b, lambd, 2)\n",
    "print(\"Cost: \", A)\n",
    "# Accuracy\n",
    "A = ComputeAccuracy(X_train_normalized, y_test, W, b, 2)\n",
    "print(\"Accuracy: \", A)\n",
    "\n",
    "# Compute gradients\n",
    "lmb = 0.05\n",
    "grad_Ws, grad_bs = ComputeGradients(act_vals, X_train_normalized, \n",
    "                                                      Y_train,P,\n",
    "                                                      W,lmb, 2)\n",
    "# Compare numerical gradients with analytical\n",
    "threshold = 1e-5\n",
    "lambd = 0\n",
    "W_sub = [sub_W[:, 0:20] for sub_W in W]\n",
    "h_sub = [sub_H[0:20, :] for sub_H in act_vals]\n",
    "b_sub = [sub_b[:, 0:20] for sub_b in b]\n",
    "\n",
    "\n",
    "hej = list()\n",
    "\n",
    "count = 1\n",
    "for w in W:\n",
    "    if count == 1:\n",
    "        hej.append(w[:, 0:20])\n",
    "    else:\n",
    "        hej.append(w)\n",
    "    count += 1\n",
    "\n",
    "W_small, b_small = GetWeightAndBias(X_train_normalized[0:20, [0]],Y_train[:, [0]], 2, m=50)\n",
    "CompareGradients(X_train_normalized[0:20, [0]],Y_train[:, [0]],\n",
    "                 hej, b, lambd, 2, threshold)\n",
    "#CompareGradients(X_train_normalized[0:20, [0]],Y_train[:, [0]],\n",
    " #                hej, b, lambd, 2, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
