{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "\n",
    "# Matlab -> Python functions\n",
    "\n",
    "###############################################################\n",
    "\n",
    "# Loades an entire batch\n",
    "def LoadBatch(filename):\n",
    "\t\"\"\" Copied from the dataset website \"\"\" \n",
    "\twith open('Datasets/'+filename, 'rb') as fo:\n",
    "\t\tdict = pickle.load(fo, encoding='bytes') \n",
    "\treturn dict\n",
    "\n",
    "# Calculate softmax for class class estimation of each image (vector)\n",
    "def softmax(x):\n",
    "\t\"\"\" Standard definition of the softmax function \"\"\"\n",
    "\texp_x = np.exp(x)\n",
    "\treturn exp_x / np.sum(exp_x, axis=0)\n",
    "\n",
    "def ComputeGradsNum(W1, W2, b1, b2, X, Y,lambd, h=0.00001):\n",
    "    \n",
    "    grad_W2 = np.zeros(shape=W2.shape)\n",
    "    grad_b2 = np.zeros(shape=b2.shape)\n",
    "    grad_W1 = np.zeros(shape=W1.shape)\n",
    "    grad_b1 = np.zeros(shape=b1.shape)   \n",
    "    c = ComputeCost(X, Y, W1, W2,b1, b2, lambd)\n",
    "    \n",
    "    for i in range(b1.shape[0]):\n",
    "        b1_try = b1.copy()\n",
    "        b1_try[i,0] = b1_try[i,0]+h\n",
    "        c2 = ComputeCost(X, Y, W1, W2,b1_try, b2, lambd)\n",
    "        grad_b1[i,0] = (c2-c)/h\n",
    "    \n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            W1_try = W1.copy()\n",
    "            W1_try[i,j] = W1_try[i,j]+h\n",
    "            c2 = ComputeCost(X, Y, W1_try, W2,b1, b2, lambd)\n",
    "            grad_W1[i,j] = (c2-c)/h\n",
    "    \n",
    "    for i in range(b2.shape[0]):\n",
    "        b2_try = b2.copy()\n",
    "        b2_try[i,0] = b2_try[i,0]+h\n",
    "        c2 = ComputeCost(X, Y, W1,W2,b1,  b2_try, lambd)\n",
    "        grad_b2[i,0] = (c2-c)/h\n",
    "    \n",
    "    for i in range(W2.shape[0]):\n",
    "        for j in range(W2.shape[1]):\n",
    "            W2_try = W2.copy()\n",
    "            W2_try[i,j] = W2_try[i,j]+h\n",
    "            c2 = ComputeCost(X, Y, W1, W2_try,b1, b2, lambd)\n",
    "            grad_W2[i,j] = (c2-c)/h\n",
    "    \n",
    "    return grad_W1,grad_W2,grad_b1,grad_b2\n",
    "\n",
    "# Allows for efficiently view the images in a directory or \n",
    "# in a *Matlab* array or cell array\n",
    "def montage(W):\n",
    "\t\"\"\" Display the image for each label in W \"\"\"\n",
    "\timport matplotlib.pyplot as plt\n",
    "\tfig, ax = plt.subplots(2,5)\n",
    "\tfor i in range(2):\n",
    "\t\tfor j in range(5):\n",
    "\t\t\tim  = W[i*5+j,:].reshape(32,32,3, order='F')\n",
    "\t\t\tsim = (im-np.min(im[:]))/(np.max(im[:])-np.min(im[:]))\n",
    "\t\t\tsim = sim.transpose(1,0,2)\n",
    "\t\t\tax[i][j].imshow(sim, interpolation='nearest')\n",
    "\t\t\tax[i][j].set_title(\"y=\"+str(5*i+j))\n",
    "\t\t\tax[i][j].axis('off')\n",
    "\tplt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "X_train, Y_train, y_test = ReadData('data_batch_1')\n",
    "X_val_train, Y_val_train, y_val_test = ReadData('data_batch_2')\n",
    "\n",
    "#X_batch_1, Y_b1, y_b1 = ReadData('data_batch_1')\n",
    "#X_batch_2, Y_b2, y_b2 = ReadData('data_batch_2')\n",
    "#X_batch_3, Y_b3, y_b3 = ReadData('data_batch_3')\n",
    "#X_batch_4, Y_b4, y_b4 = ReadData('data_batch_4')\n",
    "#X_batch_5, Y_b5, y_b5 = ReadData('data_batch_5')\n",
    "\n",
    "#X_train = np.stack((X_batch_1, X_batch_2, X_batch_3, X_batch_4, X_batch_5))\n",
    "#Y_train = np.stack((Y_b1, Y_b2, Y_b3, Y_b4, Y_b5))\n",
    "#y_train = np.stack((y_b1, y_b2, y_b3, y_b4, y_b5))\n",
    "\n",
    "#X_train, X_val_train = X_train[:, 100]\n",
    "X_test_train, Y_test_train, y_test_test = ReadData('test_batch')\n",
    "\n",
    "# Gets mean and std of training data\n",
    "X_mean, X_std = GetMeanAndStd(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "\n",
    "# My functions\n",
    "\n",
    "###############################################################\n",
    "\n",
    "# Read pixel data, labels (classes), one-hot rep. of labels (classes)\n",
    "# Divide pixel data by 255 for correct format\n",
    "def ReadData(filename):\n",
    "    data_batch = LoadBatch(filename)\n",
    "    pixel_data = data_batch[b'data'].T\n",
    "    labels = data_batch[b'labels']\n",
    "    one_hot = np.eye(10)[labels].T\n",
    "    return pixel_data, one_hot, labels \n",
    "\n",
    "# Normalize w.r.t. training data mean and standard deviation\n",
    "# Normalization of input so that the inputs are at a comparable range\n",
    "def Normalize(train, validation,test=None):\n",
    "        train=np.float64(train)\n",
    "        validation=np.float64(validation)\n",
    "        \n",
    "        mean_X =train.mean(axis=1)\n",
    "\n",
    "        std_X=train.std(axis=1)\n",
    "\n",
    "        train=train-mean_X[:,None]\n",
    "\n",
    "        train=train/std_X[:,None]\n",
    "        validation=validation-mean_X[:,None]\n",
    "        validation=validation/std_X[:,None]\n",
    "        \n",
    "        if(test is not None):\n",
    "            test=np.float64(test)\n",
    "            test=test-mean_X[:,None]\n",
    "            test=test/std_X[:,None]\n",
    "            return train,validation,test;\n",
    "        \n",
    "        return train,validation;\n",
    "\n",
    "# First init of model params W(eights) and b(ias)\n",
    "# Init done with 0 mean and 1 / sqrt of d and m\n",
    "# Random seed for selecting the same rndm numbers for each execution\n",
    "def GetWeightAndBias(X, Y, m=50):\n",
    "    \n",
    "    weights = list()\n",
    "    bias = list()\n",
    "    d = X.shape[0]\n",
    "    k = 10\n",
    "\n",
    "    std_d = 1 / np.sqrt(d)\n",
    "    std_m = 1 / np.sqrt(m)\n",
    "    \n",
    "    # W1 = m (50) x d (3072)\n",
    "    # W2 = K (10) x m (50)\n",
    "    np.random.seed(400)\n",
    "    weights.append(np.random.normal(loc=0.0, scale=std_d, size=(m, d)))\n",
    "    weights.append(np.random.normal(loc=0.0, scale=std_m, size=(k, m)))\n",
    "        \n",
    "    # b1 = m (50) x 1\n",
    "    # b2 = K (10) x 1\n",
    "    np.random.seed(400)\n",
    "    bias.append(np.random.normal(loc=0.0, scale=std_d, size=(m,1)))#np.zeros(shape=(m, 1))\n",
    "    bias.append(np.random.normal(loc=0.0, scale=std_m, size=(k,1)))#np.zeros(shape=(b_size[0], 1))\n",
    "    \n",
    "    # OLD\n",
    "    # b = np.random.normal(loc=0.0, scale=0.01, size=(b_size[0], 1))\n",
    "    # W = np.random.normal(loc=0.0, scale=0.01, size=(10, m))\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "# Evaluation of the network function\n",
    "# Agan, Softmax returns each probability for each class label\n",
    "def EvaluateClassifier(X, W1, W2,b1, b2):\n",
    "   \n",
    "    # Saving this for later\n",
    "    #activaiton_list = list() \n",
    "    # just 'hard coded' 2 layer NN for now\n",
    "    # s1 = np.dot(W[0], X) + b[0]\n",
    "    # Out put of nodes, first layer\n",
    "    # h1 = np.maximum(0, s1)\n",
    "    # s2 = np.dot(W[1], h1)+ b[1]\n",
    "    # Out put of nodes, second layer\n",
    "    \n",
    "    s1=W1@X+b1\n",
    "    #relu    \n",
    "    h1=s1 * (s1 > 0)\n",
    "    s2=W2@h1+b2\n",
    "\n",
    "    P = softmax(s2)\n",
    "    act_vals = h1\n",
    "    return P, act_vals\n",
    "\n",
    "# Total cost of a set of images:\n",
    "# 1. Regularization term, calculate: lambda * sum(W^2 ij)\n",
    "# 2. Sum it with l_cross + regularization term -> for each x,y in D\n",
    "# 3. Multiply everything with 1 / length of D\n",
    "def ComputeCost(X, Y, W1, W2, b1, b2, lambd):\n",
    "\n",
    "    # Saving for later\n",
    "    # Calculate P using softmax\n",
    "    #P, act_vals = EvaluateClassifier(X, W, b)\n",
    "    \n",
    "    # Calculate cross-entropy-loss\n",
    "    #l_cross = -np.sum(np.multiply(Y, np.log(P)))\n",
    "    \n",
    "    # Calculate regularization term\n",
    "    #sigma = lambda x, k: for i in range(k) np.sum(np.square(x))\n",
    "    \n",
    "    #not sure about this one\n",
    "    # reg_term = lambd * np.sum([np.sum(np.square(w)) for w in W])\n",
    "    \n",
    "    # Calculate total cost of the set of imgs\n",
    "    # J = (1 / len(X[1])) * l_cross + reg_term\n",
    "    \n",
    "    P, H = EvaluateClassifier(X, W1, W2, b1, b2)\n",
    "    lcr =- np.sum(np.multiply(Y, np.log(P)))\n",
    "    Reg_term = lambd*((W1**2).sum()+(W2**2).sum())\n",
    "    J = lcr/X.shape[1]+Reg_term\n",
    "    return J\n",
    "\n",
    "\n",
    "# Accuracy of the network's predictions\n",
    "# Percentage of examples for which it gets the correct answer\n",
    "def ComputeAccuracy(X, y, W1, W2, b1, b2):\n",
    "    P, act_vals = EvaluateClassifier(X,W1,  W2,b1, b2)\n",
    "    acc = np.mean(y == np.argmax(P, axis=0))\n",
    "    \n",
    "    return acc\n",
    "    \n",
    "# Compute gradients of the cost function to see the curve of cost decrease \n",
    "# Forward pass is already done since we have already calculated P\n",
    "def ComputeGradients(act_vals, X, Y, P, W1, W2,lambd):\n",
    "\n",
    "    n_b1 = X.shape[1]\n",
    "    K = Y.shape[0]\n",
    "    d = act_vals.shape[0]\n",
    "\n",
    "    # Backward pass\n",
    "    G_batch = -(Y - P)\n",
    "    \n",
    "    # Backward pass for W + reg term fix reg later\n",
    "    grad_W2 = np.dot(G_batch, act_vals.T)/ n_b1 + 2 * lambd * W2\n",
    "        \n",
    "    # Backward pass for b\n",
    "    grad_b2=(np.dot(G_batch,np.ones(shape=(n_b1,1)))/n_b1).reshape(K,1)\n",
    "\n",
    "    # Backward pass second layer\n",
    "    G_batch = W2.T @ G_batch\n",
    "\n",
    "    # g_test = G_batch * act_vals.T\n",
    "    \n",
    "    # 1 as in use this node \n",
    "    G_batch = G_batch*(act_vals>0) #, where=(act_vals >0))\n",
    "    \n",
    "    grad_W1 = np.dot(G_batch, X.T) / n_b1 + 2 * lambd * W1\n",
    "    grad_b1=(np.dot(G_batch,np.ones(shape=(n_b1,1)))/n_b1).reshape(d,1)\n",
    "\n",
    "\n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "\n",
    "# Check if my analytical gradients \n",
    "# Using centered difference function\n",
    "# If the differenc is < 1e-6, the analytical gradients are fine\n",
    "def CompareGradients(act_vals, X,Y, W1, W2, b1,b2, lambd, threshold):\n",
    "    \n",
    "    P, act_vals = EvaluateClassifier(X, W1, W2, b1,b2)\n",
    "\n",
    "    #Calculate gradients\n",
    "    grad_W1_a, grad_W2_a, grad_b1_a, grad_b2_a = ComputeGradients(act_vals, X, Y,P, W1,W2, lambd)\n",
    "    grad_W1_n,grad_W2_n, grad_b1_n, grad_b2_n = ComputeGradsNum(W1, W2, b1,b2, X, Y,lambd, h=0.00001)\n",
    "\n",
    "    # Calculate differences\n",
    "    w_rel_error_1 = np.sum(np.abs(grad_W1_a - grad_W1_n)) / np.maximum(0.001, np.sum(np.abs(grad_W1_a) + np.abs(grad_W1_n)))\n",
    "    w_rel_error_2 = np.sum(np.abs(grad_W2_a - grad_W2_n)) / np.maximum(0.001, np.sum(np.abs(grad_W2_a) + np.abs(grad_W2_n)))\n",
    "\n",
    "    b_rel_error_1 = np.sum(np.abs(grad_b1_a - grad_b1_n)) / np.maximum(0.001, np.sum(np.abs(grad_b1_a) + np.abs(grad_b1_n)))\n",
    "    b_rel_error_2 = np.sum(np.abs(grad_b2_a - grad_b2_n)) / np.maximum(0.001, np.sum(np.abs(grad_b2_a) + np.abs(grad_b2_n)))\n",
    "\n",
    "    # Check differences\n",
    "    if (w_rel_error_1 and w_rel_error_2) and (b_rel_error_2 and b_rel_error_1) < threshold:\n",
    "        print(\"Analytical ok\")\n",
    "    else:\n",
    "        print(\"Gradient difference too high\")\n",
    "\n",
    " \n",
    "def MiniBatchGD2(X, Y, y, GDparams, W1, W2, b1, b2, X_val=None, Y_val=None, y_val=None, lambd= 0 ):\n",
    "    n = X.shape[1]\n",
    "    (eta_min,eta_max,step_size,n_batch,cycles)=GDparams\n",
    "    metrics = {'updates':[-1], \n",
    "               'Loss_scores':[ComputeCost(X, Y, W1, W2, b1, b2, lambd)], \n",
    "               'acc_scores':[ComputeAccuracy(X, y, W1, W2, b1, b2)]}\n",
    "    if X_val is not None:\n",
    "        metrics['Loss_val_scores'] = [ComputeCost(X_val, Y_val, W1, W2,b1, b2, lambd)]\n",
    "        metrics['acc_val_scores'] = [ComputeAccuracy(X_val, y_val, W1, W2, b1, b2)]\n",
    "    batches = dict()\n",
    "\n",
    "    for j in range(n//n_batch):\n",
    "            j_start = (j)*n_batch ;\n",
    "            j_end = (j+1)*n_batch;\n",
    "            inds = range(j_start,j_end);\n",
    "            y_batch = [y[index] for index in inds]\n",
    "            X_batch = X[:, inds];\n",
    "            Y_batch = Y[:, inds];\n",
    "            batches[j]=(X_batch,Y_batch,y_batch)\n",
    "    j = 0\n",
    "    \n",
    "    for l in range(cycles):\n",
    "        for t in range(2*l*step_size, 2*(l+1)*step_size):\n",
    "            \n",
    "            if t>= 2*l*step_size and t<(2*l+1)*step_size:\n",
    "                eta = eta_min+(t-2*l*step_size)/step_size*(eta_max-eta_min)\n",
    "            elif t>=(2*l+1)*step_size and t<2*(l+1)*step_size:\n",
    "                eta = eta_max-(t-(2*l+1)*step_size)/step_size*(eta_max-eta_min)\n",
    "\n",
    "            X_batch, Y_batch, y_batch = batches[j]\n",
    "            P_batch, H_batch = EvaluateClassifier(X_batch, W1, W2,b1, b2)\n",
    "            grad_W1, grad_W2, grad_b1, grad_b2 = ComputeGradients(H_batch, X_batch, Y_batch, P_batch, W1, W2,lambd)\n",
    "\n",
    "          #  print(W1)\n",
    "           # if(math.isnan(W1[0][0])):\n",
    "           #     print(\"NU ÄR DET NAN \")\n",
    "           #     print(\"J = \", j)\n",
    "           #     print(W1)\n",
    "             #   hej()\n",
    "                # .1 * 1.5\n",
    "                # .0001 * 1.5\n",
    "            W1 -= eta*grad_W1\n",
    "            b1 -= eta*grad_b1\n",
    "            W2 -= eta*grad_W2\n",
    "            b2 -= eta*grad_b2\n",
    "            j += 1\n",
    "            if j>(n//n_batch-1):\n",
    "                # set j = 0 will start new epoch\n",
    "                j = 0\n",
    "                metrics['updates'].append(t+1)\n",
    "                metrics['acc_scores'].append(ComputeAccuracy(X, y, W1, W2, b1, b2))\n",
    "                metrics['Loss_scores'].append(ComputeCost(X, Y, W1, W2, b1, b2,lambd))\n",
    "\n",
    "                if X_val is not None:\n",
    "                    metrics['acc_val_scores'].append(ComputeAccuracy(X_val, y_val, W1, W2,b1, b2))\n",
    "                    metrics['Loss_val_scores'].append(ComputeCost(X_val, Y_val, W1, W2,b1, b2, lambd))\n",
    "                message = \"In update \"+str(t+1)+'/'+str(2*cycles*step_size)+\" finishes epoch \"+ \\\n",
    "                          str(len(metrics['updates'])-1)+\": loss=\"+str(metrics['Loss_scores'][-1])+ \\\n",
    "                          \" and accuracy=\"+str(metrics['acc_scores'][-1])+\" (training set) \\r\"\n",
    "                sys.stdout.write(message)\n",
    "            \n",
    "        \n",
    "    \n",
    "    return W1, b1, W2, b2, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all data w.r.t. mean and std of training data\n",
    "X_train_normalized, X_val_train_normalized, X_test_train_normalized = Normalize(X_train, X_val_train, X_test_train, X_mean, X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 3072)\n",
      "(10, 50)\n",
      "(50, 1)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create model params W and b\n",
    "W, b = GetWeightAndBias(X_train_normalized, Y_train, m=50)\n",
    "W1 = W[0]\n",
    "W2 = W[1]\n",
    "b1 = b[0]\n",
    "b2 = b[1]\n",
    "\n",
    "print(W1.shape)\n",
    "print(W2.shape)\n",
    "print(b1.shape)\n",
    "print(b2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation (take softmax)\n",
    "P, act_vals = EvaluateClassifier(X_train_normalized, W1, W2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = dxn array of images (columns)\n",
    "# Y = 1xn vector of labels (in one-hot) for X\n",
    "# J = scalar corresponing to sum of the loss of the network's predictions,\n",
    "# in X relative to ground truth labels and reg. term on W. \n",
    "# Lambda = specifies how much penalty to be added \n",
    "J = ComputeCost(X_train_normalized, Y_train, W1, W2, b1, b2, lambd = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ComputeAccuracy(X_train_normalized, y_test, W1, W2, b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "lmb = 0.05\n",
    "grad_W1, grad_W2, grad_b1, grad_b2 = ComputeGradients(act_vals, X_train_normalized, \n",
    "                                                      Y_train,P,\n",
    "                                                      W1, W2, lmb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical ok\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-5\n",
    "\n",
    "CompareGradients(act_vals, X_train_normalized[0:20, [0]],Y_train[:, [0]], W1[:, 0:20], W2, b1,b2, 0, threshold)\n",
    "\n",
    "#CompareGradients(X_train[0:1000, [1]], Y_train[:, [1]], W[:, 0:1000], b, 0, threshold)\n",
    "#CompareGradients(X_train[0:3072, [1]], Y_train[:, [1]], W[:, 0:3072], b, 0, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "(50, 1)\n",
      "(10, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-463-a7896c16034b>:17: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
      "<ipython-input-463-a7896c16034b>:17: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
      "<ipython-input-463-a7896c16034b>:243: RuntimeWarning: invalid value encountered in greater\n",
      "  G_batch = G_batch*(act_vals>0) #, where=(act_vals >0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'updates': [-1, 1000, 2000], 'Loss_scores': [2.543752870781795, nan, nan], 'acc_scores': [0.0898, 0.1005, 0.1005], 'Loss_val_scores': [2.533495633039159, nan, nan], 'acc_val_scores': [0.092, 0.0984, 0.0984]}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "#GDparams = {'n_batch': 10, 'eta_min': 1e-5, 'eta_max':1e-1, 'cycles': 2}\n",
    "lambd = 0\n",
    "\n",
    "eta_min=1e-5\n",
    "eta_max=1e-1\n",
    "step_size=500\n",
    "n_batch=10\n",
    "cycles=2\n",
    "GDparams=(eta_min,eta_max,step_size,n_batch,cycles)\n",
    "\n",
    "# Train data\n",
    "#cost, accuracy_list, s_im, W_upd, b_upd = MiniBatchGD(X_train_normalized, Y_train, GDparams, W, b, lambd)\n",
    "print(X_train_normalized.dtype)\n",
    "# Validation data\n",
    "print(b1.shape)\n",
    "print(b2.shape)\n",
    "W1_upd, b1_upd, W2_upd, b2_upd, metrics = MiniBatchGD2(X_train_normalized, Y_train, y_test, GDparams, \n",
    "                                                       W1, W2, b1, b2,  \n",
    "                                                       X_val_train_normalized, \n",
    "                                                       Y_val_train, y_val_test, lambd)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots()  \n",
    "ax.plot(np.arange(GDparams['n_epochs']), cost, 'b', label='Training loss')  \n",
    "ax.plot(np.arange(GDparams['n_epochs']), cost_val, 'r', label='Validation loss') \n",
    "ax.set_xlabel('Iterations')  \n",
    "ax.set_ylabel('Cost')  \n",
    "ax.set_title('Error vs. Training Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Current lambda:\", lambd) \n",
    "print(\"Current n_batch:\", GDparams['n_batch'])\n",
    "print(\"Current eta:\", GDparams['eta'])\n",
    "print(\"Current n_epochs:\", GDparams['n_epochs'])\n",
    "print(\"Accuracy test data: \", Acc)\n",
    "\n",
    "# Visualization of weight matrix W for each epoch\n",
    "montage(W_upd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(data_train, label_names):\n",
    "    weights = list()\n",
    "    bias = list()\n",
    "    weights.append(np.random.normal(0, 1 / np.sqrt(data_train.shape[0]),\n",
    "                                    (HIDDEN_NODES, data_train.shape[0])))  # Dim: m x d\n",
    "    weights.append(np.random.normal(0, 1 / np.sqrt(HIDDEN_NODES),\n",
    "                                    (len(label_names), HIDDEN_NODES)))  # Dim: k x m\n",
    "    bias.append(np.zeros((HIDDEN_NODES, 1)))  # Dim: m x 1\n",
    "    bias.append(np.zeros((len(label_names), 1)))  # Dim: k x 1\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "\n",
    "def forward_pass(data_train, weights, bias):\n",
    "    output = list()  # Output of previous layer list\n",
    "    s_list = list()  # s values list\n",
    "    output.append(np.copy(data_train))\n",
    "    s_list.append(compute_s(data_train, weights[0], bias[0]))\n",
    "    for i in range(1, len(weights)):\n",
    "        output.append(compute_h(s_list[-1]))\n",
    "        s_list.append(compute_s(output[-1], weights[i], bias[i]))\n",
    "\n",
    "    return output, s_list\n",
    "\n",
    "\n",
    "def cyclical_update(t, n_s, eta_min, eta_max):\n",
    "    cycle = int(t / (2 * n_s))  # Number of complete cycles elapsed\n",
    "    if 2 * cycle * n_s <= t <= (2 * cycle + 1) * n_s:\n",
    "        return eta_min + (t - 2 * cycle * n_s) / n_s * (eta_max - eta_min)\n",
    "    if (2 * cycle + 1) * n_s <= t <= 2 * (cycle + 1) * n_s:\n",
    "        return eta_max - (t - (2 * cycle + 1) * n_s) / n_s * (eta_max - eta_min)\n",
    "\n",
    "\n",
    "def softmax(s):\n",
    "    return np.exp(s) / np.sum(np.exp(s), axis=0)\n",
    "\n",
    "\n",
    "def compute_h(s):\n",
    "    return np.maximum(0, s)\n",
    "\n",
    "\n",
    "def l_cross(y, p):\n",
    "    return -np.log(np.sum(y * p, axis=0))\n",
    "\n",
    "\n",
    "def compute_s(data, weight, bias):\n",
    "    s = weight @ data + bias  # Dim: k x n\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def compute_loss(data, labels, weights, bias):\n",
    "    p = softmax(forward_pass(data, weights, bias)[1][-1])  # Value of s_2 computed in the forward pass\n",
    "    l_cross_sum = np.sum(l_cross(labels, p))\n",
    "\n",
    "    return (1 / data.shape[1]) * l_cross_sum\n",
    "\n",
    "\n",
    "def compute_cost(data, labels, weights, bias, lmb):\n",
    "    loss = compute_loss(data, labels, weights, bias)\n",
    "    reg = lmb * np.sum([np.sum(np.square(w)) for w in weights])  # Regularization term L2\n",
    "\n",
    "    return loss + reg\n",
    "\n",
    "\n",
    "def compute_accuracy(data, labels, weights, bias):\n",
    "    p = softmax(forward_pass(data, weights, bias)[1][-1])  # Value of s_2 computed in the forward pass\n",
    "    prediction = np.argmax(p, axis=0)\n",
    "    real = np.argmax(labels, axis=0)\n",
    "\n",
    "    return np.sum(real == prediction) / len(real)\n",
    "\n",
    "\n",
    "def compute_grads_analytic(data, labels, weights, lmb, p):\n",
    "    grad_weights = list()\n",
    "    grad_bias = list()\n",
    "    # Last layer --> data[0] is the original input\n",
    "    g = -(labels - p)  # Dim: k x n\n",
    "    grad_weights.append((g @ data[-1].T) / data[0].shape[1] + 2 * lmb * weights[-1])\n",
    "    grad_bias.append(np.sum(g, axis=1)[:, np.newaxis] / data[0].shape[1])\n",
    "    # Remaining layers\n",
    "    for i in reversed(range(len(data) - 1)):  # Reverse traversal of the lists\n",
    "        g = weights[i + 1].T @ g  # Multiply by previous weight\n",
    "        diag = np.copy(data[i + 1])  # Perform a copy of the output of the previous layer\n",
    "        diag[diag > 0] = 1  # Transform every element > 0 into 1\n",
    "        # diag[diag < 0] = 0  # Transform every element < 0 into 0\n",
    "        g = g * diag  # Element multiplication by diagonal of the indicator over data[i]\n",
    "        grad_weights.append((g @ data[i].T) / data[0].shape[1] + 2 * lmb * weights[i])\n",
    "        grad_bias.append(np.sum(g, axis=1)[:, np.newaxis] / data[0].shape[1])\n",
    "    grad_weights.reverse(), grad_bias.reverse()  # Reverse lists to return the same order\n",
    "\n",
    "    return grad_weights, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
